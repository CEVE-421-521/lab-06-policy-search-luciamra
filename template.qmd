---
title: "Lab 6: Policy Search"
author: "Lucia Roemro-Alston (lmr12)"
jupyter: julia-1.10
date: 2024-03-01
week: 7
categories: [Lab]

format: 
    html: default

    # YOU DO NOT NEED BOTH PDF AND DOCX.
    # COMMENT OR DELETE THE ONE YOU DON'T WANT TO USE.
    pdf:
        documentclass: article
        fontsize: 11pt
        geometry:
            - margin=1in  
        number-sections: true
        code-line-numbers: true
    #docx: 
    #    toc: true
    #    fig-format: png
    #    number-sections: true
    #    code-line-numbers: true

date-format: "ddd., MMM. D"

execute: 
  cache: true
  freeze: auto

bibliography: references.bib
---

```{julia}
using Revise
using HouseElevation

using CSV
using DataFrames
using DataFramesMeta
using Distributions
using LaTeXStrings
using Metaheuristics
using Plots
using Random
using Unitful

Plots.default(; margin=5Plots.mm)
```

```{julia}
function objective_function(a::AbstractFloat)
    return true # PLACEHOLDER
end
```

## Objective Function

We want to keep the same objective function that we have been using in order to find the net present value taking in the cost of heightening the house and the discounted expected costs of future flood damage. We also consider and define the states of the world over which we will analyze.
The building I am studying is Fisherman's Wharf at 2200 Harborside Drive Galveston, TX.
I got the information for the building area from the cvent website for event space. The value of the property results from searches on Zillow for average building prices in the area. Finally, the depth-damage curve which I chose is a result of location, building type, and what I am looking to analyze for damages. 
```{julia}
slr_scenarios = let
    df = CSV.read("data/slr_oddo.csv", DataFrame)
    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]
end

house = let
    haz_fl_dept = CSV.read("data/haz_fl_dept.csv", DataFrame) # read in the file
    desc = "Cafeteria Restaurant, structure"
    row = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want
    area = 4004u"ft^2"
    height_above_gauge = 4*u"ft"
    House(row; area=area, height_above_gauge=height_above_gauge, value_usd=400_000)
end

p = ModelParams(; house=house, years=2024:2100)

function draw_surge_distribution()
    μ = rand(Normal(5, 1))
    σ = rand(Exponential(1.5))
    ξ = rand(Normal(0.1, 0.05))
    return GeneralizedExtremeValue(μ, σ, ξ)
end
function draw_discount_rate()
    return 0.0
end

N_SOW = 100_000
sows = [
    SOW(rand(slr_scenarios), draw_surge_distribution(), draw_discount_rate()) for
    _ in 1:N_SOW
] # for 100,000 SOWs

N_SOW_opt = 10
sows_opt = collect(Iterators.take(sows, 10))
```

## Validation

We can validate our answer by comparing it to a simple Monte Carlo estimate using 25,000 samples and timing how long this takes us. 
```{julia}
a = Action(3.0u"ft")
sow = first(sows)
_ = run_sim(a, sow, p)
@time run_sim(a, sow, p)
```

```{julia}
_ = HouseElevation.run_sim_old(a, sow, p)
@time HouseElevation.run_sim_old(a, sow, p)
```

## Optomization Problem Example

We are looking to solve a non-linear, non-convex optomization problem using the Metaheuristics.jl package.
The following plots the function we are looking to minimize for D=2.
```{julia}
f(x) = 10length(x) + sum(x .^ 2 - 10cos.(2π * x))
let
    # Generate a grid of points for the surface plot
    x = range(-5; stop=5, length=1000)
    y = range(-5; stop=5, length=1000)
    z = [f([i, j]) for i in x, j in y]

    # Create the surface plot
    surface(
        x, y, z; xlabel="x1", ylabel="x2", zlabel="f(x)", title=L"Minimize $f(x)$ for $D=2$"
    )
end
```

Now, if we want to minimize with D=10, we have to define bounds that constrians the search space to the decision variables.
```{julia}
D = 10
bounds = boxconstraints(; lb=-5ones(D), ub=5ones(D))
```

This can be optomized using the optomize function.
```{julia}
result = optimize(f, bounds)
```

The minimum of the objective function:
```{julia}
minimum(result)
```


The value of the decision variable that achieves the minimum:
```{julia}
minimizer(result)
```

It can be helpful to set constraints and specifications to the optomizer such as time limits, especially for problems that may take very long to run.
```{julia}
options = Options(; time_limit=10.0)
```

 We will use the ECA algorithm which is suggested as default.
 ```{julia}
 algorithm = ECA(; options=options)
 ```

 We will set a random seed to make our results more reproducable and to understand the sensitivity of our results with respect to the random seed.
 ```{julia}
 Random.seed!(918)
result = optimize(f, bounds, algorithm)
```

And check for a different iteration:
```{julia}
Random.seed!(952)
result = optimize(f, bounds, algorithm)
```

## Varying D

We can play around with some of the conditions of the optomization above in order to gain some knowledge on the effects of certian values on the optomization. For example, we can vary D to develop our understanding of this parameter.
```{julia}
function optomization()
    trials = 1:2:19
    optomization_res = []
    for D in trials
        bounds = boxconstraints(; lb=-5ones(D), ub=5ones(D))
        # Optomization:
        result = optimize(f, bounds)
        # Minimum:
        minimum(result)
        # Value of the decision variable that achieves the minimum:
        minimizer(result)
        # Specifications:
        options = Options(; time_limit=10.0)
        algorithm = ECA(; options=options)
        # Random Seed:
        Random.seed!(918)
        result2 = optimize(f, bounds, algorithm)
        push!(optomization_res, result2)
    end
    return optomization_res
end

x =optomization()
```

From this analysis with D as the varying parameter, one general trend is an increase in the amount of iterations as the value of D increaes. There are some deviations to this trend, but for the most part this correlation is strong. Another interesting observation is the increase in the number of decision variables that result in the minimum value as the value for D increases. This means that there are more options for an ideal scenario that cuases minimization. The number of function calls is another result that increases along with the value of D in thsi analysis. Interestingly, the value of the minimum has no consistent trend, and the highest values of it occur around D values between 13 and 15.

## Optimization

In order to optimize our problem we must define an objective function that will include the objective and the SOWs over which we will analyse the problem.
```{julia}
# Sea Level Rise Scenarios:
slr_scenarios = let
    df = CSV.read("data/slr_oddo.csv", DataFrame)
    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]
end

# Flood Depth and Home-Specific Information:
house = let
    haz_fl_dept = CSV.read("data/haz_fl_dept.csv", DataFrame) # read in the file
    desc = "Cafeteria Restaurant, structure"
    row = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want
    area = 4004u"ft^2"
    height_above_gauge = 4*u"ft"
    House(row; area=area, height_above_gauge=height_above_gauge, value_usd=400_000)
end

p = ModelParams(; house=house, years=2024:2100)

# Surge Distribiton:
function draw_surge_distribution()
    μ = rand(Normal(5, 1))
    σ = rand(Exponential(1.5))
    ξ = rand(Normal(0.1, 0.05))
    return GeneralizedExtremeValue(μ, σ, ξ)
end
# Discount Rate:
function draw_discount_rate()
    return 0.0
end

# Combination of information above to create different states of the world:
N_SOW = 100_000
sows = [
    SOW(rand(slr_scenarios), draw_surge_distribution(), draw_discount_rate()) for
    _ in 1:N_SOW
] # for 100,000 SOWs

# taking the first 10 SOWs drom those generated above
N_SOW_opt = 10
sows_opt = collect(Iterators.take(sows, 10))
```

This is the objective function which will take in the SOW information from the cell above and use it to analyze a sat of actions, or home elevations over all of these SOWs. This will provide information on how a given home elvation will act across "all" scenarios. We then take the negative sum of these values to have a comparison over which to consider different actions.
```{julia}
function objective(a::Vector{Float64})
    action = Action(a[1]*u"ft")

    # finding the npv of each N_SOW_opt
    npv_each = [run_sim(action,s,p) for s in sows_opt]
    # the negative sum of the expected values
    min = -1.0*sum(npv_each)
    return min
end
```

Testing this function for different heights:
∆h = 1.0ft
```{julia}
a = Action(1.0u"ft")
npv_min = objective(a)
```

∆h = 7.0ft
```{julia}
a = Action(7.0u"ft")
npv_min = objective(a)
```

∆h = 13.0ft
```{julia}
a = Action(13.0u"ft")
npv_min = objective(a)
```

We can run an optimization of the objective function to find what elevation it recommends.
```{julia}
bounds = boxconstraints(; lb=0.0, ub=14.0)
# Optomization:
result = optimize(objective, bounds)
# Minimum:
minimum(result)
# Value of the decision variable that achieves the minimum:
minimizer(result)
# Specifications:
options = Options(; time_limit=10.0)
algorithm = ECA(; options=options)
# Random Seed:
Random.seed!(2024)
result2 = optimize(objective, bounds, algorithm)
```

Based on the optimization of our objective function, we are finding that the minimizing action is elevating our building to ∆h = 14.0ft. 



## Reflection

How are we framing this problem? What are the decision variables, the objective function, and the states of the world over which we optimize?

In order to optimize this problem, we are setting it up as a means to figure out which one of our actions are policies has the best results when the simulation is run on them and parameters are varied. The decision variable is how high to elevate our home, and the objective function gives us data about our home and the flooding and surge distributions surrounding my building. The states of the world over which we optimize are different situations composed of real life considerations that make up the environment for a trial of our solutions. The state of the world can include future economy (discount rate), surge situations, and sea level rise. 

Diggning deeper, we are averaging the objective function computed over a finite number of states of the world. This assumes that they are all drawn from a distribution representing the “true” distribution of states of the world. Is this a good assumption?

This is a good assumption for our current modeling capabilities and with limited computer efficiency. If we had the ability to do so, we would ideally run infinate simulations so that the function converges onto one value. Because this is not possible, we can get a good unferstanding of what would happen if this were the case and develop further knowledge on a topic that is difficult to predict.

What’s not being considered in this analysis that might be important?

One thing that is not being considered in this analysis that can have a significant imppact is fwater contamination and quality in our home's area. This can affect the way that the flooding intereacts with the home and as a result, the damages produced, making home elevation more or less important. There are also more personal and individual influences to whether or not someone will elevate their home like work promotions or job losses that can impact a persons ability to raise.